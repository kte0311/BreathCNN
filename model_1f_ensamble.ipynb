{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8246656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import torch_directml\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de495593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DirectML device: privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "if torch_directml.is_available():\n",
    "    device = torch_directml.device()\n",
    "    print(f\"DirectML device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"DirectML not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29750748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogMelAudioDataset(Dataset):\n",
    "    def __init__(self, npy_log_mel_base_dir, file_names_list, labels_list, mean=None, std=None):\n",
    "        self.npy_log_mel_base_dir = npy_log_mel_base_dir\n",
    "        self.file_names = file_names_list\n",
    "        self.labels = labels_list\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name_id = self.file_names[idx]\n",
    "        log_mel_path = os.path.join(self.npy_log_mel_base_dir, file_name_id + '.npy')\n",
    "        try:\n",
    "            log_mel_map = np.load(log_mel_path).astype(np.float32)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"오류: 원본 데이터 파일 로드 실패 (ID: {file_name_id}) - {e}\")\n",
    "            raise e\n",
    "\n",
    "        if self.mean is not None and self.std is not None:\n",
    "            log_mel_map = (log_mel_map - self.mean) / self.std\n",
    "\n",
    "        log_mel_tensor = torch.from_numpy(log_mel_map).float().unsqueeze(0)\n",
    "\n",
    "        if self.is_train and self.spec_augment is not None:\n",
    "            log_mel_tensor = self.spec_augment(log_mel_tensor)\n",
    "\n",
    "        if not self.is_test:\n",
    "            label = self.labels[idx]\n",
    "            label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "            return log_mel_tensor, label_tensor\n",
    "        else:\n",
    "            return log_mel_tensor, file_name_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca06132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleBranchLogMelCNN(nn.Module):\n",
    "    def __init__(self, num_classes,\n",
    "                 input_channels, input_height, input_width,\n",
    "                 fc_hidden_dim=256,\n",
    "                 dropout_rate=0.5):\n",
    "        super(SingleBranchLogMelCNN, self).__init__()\n",
    "        self.target_height = 16\n",
    "        self.target_width = 6\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.adaptive_pool = nn.AdaptiveMaxPool2d((self.target_height, self.target_width))\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, input_channels, input_height, input_width)\n",
    "            x = self.pool1(self.relu1(self.conv1(dummy_input)))\n",
    "            x = self.pool2(self.relu2(self.conv2(x)))\n",
    "            x = self.relu3(self.conv3(x))\n",
    "            out = self.adaptive_pool(x)\n",
    "            self.flattened_size = out.numel()\n",
    "        self.classifier_fc = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, fc_hidden_dim),\n",
    "            nn.BatchNorm1d(fc_hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(fc_hidden_dim, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.relu1(self.conv1(x)))\n",
    "        out = self.pool2(self.relu2(self.conv2(out)))\n",
    "        out = self.relu3(self.conv3(out))\n",
    "        out = self.adaptive_pool(out)\n",
    "        out = out.view(-1, self.flattened_size)\n",
    "        output = self.classifier_fc(out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8502e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_stats(file_names_list, base_dir):\n",
    "    all_values_sum = 0.0\n",
    "    all_values_sum_sq = 0.0\n",
    "    total_elements_count = 0\n",
    "    print(f\"'{base_dir}'에서 통계치 계산 중 ({len(file_names_list)}개 파일)...\")\n",
    "    for fname_id in file_names_list:\n",
    "        path = os.path.join(base_dir, fname_id + '.npy')\n",
    "        try:\n",
    "            data = np.load(path).astype(np.float64)\n",
    "            all_values_sum += np.sum(data)\n",
    "            all_values_sum_sq += np.sum(data**2)\n",
    "            total_elements_count += data.size\n",
    "        except FileNotFoundError: continue\n",
    "        except Exception as e: continue\n",
    "\n",
    "    if total_elements_count == 0:\n",
    "        return 0.0, 1.0\n",
    "        \n",
    "    mean = all_values_sum / total_elements_count\n",
    "    variance = (all_values_sum_sq / total_elements_count) - (mean**2)\n",
    "    std = np.sqrt(max(variance, 1e-12))\n",
    "    std = max(std, 1e-8)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12ab4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0, path='best_model_checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "        self.best_model_saved = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            # print(f\"Validation loss decreased ({self.best_loss:.6f}). Saving model to {self.path}\")\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "            self.best_model_saved = True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            # print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d0f653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 4000개의 학습/검증 샘플 정보를 CSV 파일에서 로드 완료.\n",
      "\n",
      "[추론용] 전체 학습 데이터에 대한 정규화 통계치를 계산합니다...\n",
      "'C:\\\\머신러닝\\\\kaggle2\\\\kaggle\\train_log_mel'에서 통계치 계산 중 (4000개 파일)...\n",
      "--- 계산 완료: Mean=-34.6940, Std=20.1266 ---\n"
     ]
    }
   ],
   "source": [
    "PERSISTENT_STORAGE_PATH = \"C:\\\\\\\\머신러닝\\\\\\\\kaggle2\\\\\\\\kaggle\"\n",
    "os.makedirs(PERSISTENT_STORAGE_PATH, exist_ok=True)\n",
    "MODEL_SAVE_DIR = os.path.join(PERSISTENT_STORAGE_PATH, 'model')\n",
    "SUBMISSION_SAVE_DIR = os.path.join(PERSISTENT_STORAGE_PATH, 'submission')\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(SUBMISSION_SAVE_DIR, exist_ok=True)\n",
    "CSV_FILE_PATH = os.path.join(PERSISTENT_STORAGE_PATH, 'train.csv')\n",
    "TEST_CSV_FILE_PATH = os.path.join(PERSISTENT_STORAGE_PATH, 'test.csv')\n",
    "NPY_LOG_MEL_TRAIN_DIR_FOR_DATASET = os.path.join(PERSISTENT_STORAGE_PATH, 'train_log_mel')\n",
    "NPY_LOG_MEL_TEST_DIR_FOR_DATASET= os.path.join(PERSISTENT_STORAGE_PATH, 'test_log_mel')\n",
    "N_SPLITS = 10\n",
    "FILENAME_COLUMN_IN_CSV = \"ID\"\n",
    "TARGET_COLUMN_IN_CSV = \"Target\"\n",
    "CLASSES = [\"I\", \"E\"]\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "RANDOM_SEED = 42\n",
    "FC_HIDDEN_DIM_MODEL = 128\n",
    "DROPOUT_RATE_MODEL = 0.5\n",
    "\n",
    "# input channels 1개!!!, height, width\n",
    "INPUT_CHANNELS = 1    \n",
    "LOG_MEL_HEIGHT, LOG_MEL_WIDTH = 128, 44\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "EARLY_STOPPING_MIN_DELTA = 0.001\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# --- 데이터 준비 (CSV 로드 및 레이블 변환) ---\n",
    "try:\n",
    "    train_val_metadata_df = pd.read_csv(CSV_FILE_PATH)\n",
    "    train_val_all_ids = train_val_metadata_df[FILENAME_COLUMN_IN_CSV].tolist()\n",
    "    train_val_raw_labels = train_val_metadata_df[TARGET_COLUMN_IN_CSV].tolist()\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: 로컬 학습 CSV 파일({CSV_FILE_PATH})을 찾을 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "label_to_int = {class_name: i for i, class_name in enumerate(CLASSES)}\n",
    "train_val_all_labels = [label_to_int[str(label_str)] for label_str in train_val_raw_labels]\n",
    "print(f\"총 {len(train_val_all_ids)}개의 학습/검증 샘플 정보를 CSV 파일에서 로드 완료.\")\n",
    "\n",
    "print(\"\\n[추론용] 전체 학습 데이터에 대한 정규화 통계치를 계산합니다...\")\n",
    "overall_log_mel_mean, overall_log_mel_std = calculate_feature_stats(train_val_all_ids, NPY_LOG_MEL_TRAIN_DIR_FOR_DATASET)\n",
    "print(f\"--- 계산 완료: Mean={overall_log_mel_mean:.4f}, Std={overall_log_mel_std:.4f} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9ebde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================\n",
      "   Fold 1/10 학습 시작\n",
      "=========================\n",
      "Fold 1의 학습 데이터로 통계치를 계산합니다...\n",
      "'C:\\\\머신러닝\\\\kaggle2\\\\kaggle\\train_log_mel'에서 통계치 계산 중 (3600개 파일)...\n",
      "Fold 1 학습 완료.\n",
      "\n",
      "=========================\n",
      "   Fold 2/10 학습 시작\n",
      "=========================\n",
      "Fold 2의 학습 데이터로 통계치를 계산합니다...\n",
      "'C:\\\\머신러닝\\\\kaggle2\\\\kaggle\\train_log_mel'에서 통계치 계산 중 (3600개 파일)...\n",
      "Fold 2 학습 완료.\n",
      "\n",
      "=========================\n",
      "   Fold 3/10 학습 시작\n",
      "=========================\n",
      "Fold 3의 학습 데이터로 통계치를 계산합니다...\n",
      "'C:\\\\머신러닝\\\\kaggle2\\\\kaggle\\train_log_mel'에서 통계치 계산 중 (3600개 파일)...\n",
      "Fold 3 학습 완료.\n",
      "\n",
      "=========================\n",
      "   Fold 4/10 학습 시작\n",
      "=========================\n",
      "Fold 4의 학습 데이터로 통계치를 계산합니다...\n",
      "'C:\\\\머신러닝\\\\kaggle2\\\\kaggle\\train_log_mel'에서 통계치 계산 중 (3600개 파일)...\n",
      "Fold 4 학습 완료.\n",
      "\n",
      "=========================\n",
      "   Fold 5/10 학습 시작\n",
      "=========================\n",
      "Fold 5의 학습 데이터로 통계치를 계산합니다...\n",
      "'C:\\\\머신러닝\\\\kaggle2\\\\kaggle\\train_log_mel'에서 통계치 계산 중 (3600개 파일)...\n",
      "Fold 5 학습 완료.\n",
      "\n",
      "=========================\n",
      "   Fold 6/10 학습 시작\n",
      "=========================\n",
      "Fold 6의 학습 데이터로 통계치를 계산합니다...\n",
      "'C:\\\\머신러닝\\\\kaggle2\\\\kaggle\\train_log_mel'에서 통계치 계산 중 (3600개 파일)...\n",
      "Fold 6 학습 완료.\n",
      "\n",
      "=========================\n",
      "   Fold 7/10 학습 시작\n",
      "=========================\n",
      "Fold 7의 학습 데이터로 통계치를 계산합니다...\n",
      "'C:\\\\머신러닝\\\\kaggle2\\\\kaggle\\train_log_mel'에서 통계치 계산 중 (3600개 파일)...\n",
      "Fold 7 학습 완료.\n",
      "\n",
      "=========================\n",
      "   Fold 8/10 학습 시작\n",
      "=========================\n",
      "Fold 8의 학습 데이터로 통계치를 계산합니다...\n",
      "'C:\\\\머신러닝\\\\kaggle2\\\\kaggle\\train_log_mel'에서 통계치 계산 중 (3600개 파일)...\n",
      "Fold 8 학습 완료.\n",
      "\n",
      "=========================\n",
      "   Fold 9/10 학습 시작\n",
      "=========================\n",
      "Fold 9의 학습 데이터로 통계치를 계산합니다...\n",
      "'C:\\\\머신러닝\\\\kaggle2\\\\kaggle\\train_log_mel'에서 통계치 계산 중 (3600개 파일)...\n",
      "Fold 9 학습 완료.\n",
      "\n",
      "=========================\n",
      "   Fold 10/10 학습 시작\n",
      "=========================\n",
      "Fold 10의 학습 데이터로 통계치를 계산합니다...\n",
      "'C:\\\\머신러닝\\\\kaggle2\\\\kaggle\\train_log_mel'에서 통계치 계산 중 (3600개 파일)...\n",
      "Fold 10 학습 완료.\n",
      "\n",
      "==================== Stratified K-Fold 학습 완료 ====================\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 시드 고정 함수\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "X = np.array(train_val_all_ids)\n",
    "y = np.array(train_val_all_labels)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"\\n{'='*25}\\n   Fold {fold+1}/{N_SPLITS} 학습 시작\\n{'='*25}\")\n",
    "\n",
    "    train_filenames, val_filenames = X[train_idx], X[val_idx]\n",
    "    train_labels, val_labels = y[train_idx], y[val_idx]\n",
    "\n",
    "    print(f\"Fold {fold+1}의 학습 데이터로 통계치를 계산합니다...\")\n",
    "    # 통계치 계산은 증강되지 않은 원본 데이터로 수행해야 하므로 이 부분은 그대로 둡니다.\n",
    "    fold_mean, fold_std = calculate_feature_stats(train_filenames, NPY_LOG_MEL_TRAIN_DIR_FOR_DATASET)\n",
    "    \n",
    "\n",
    "    train_dataset = LogMelAudioDataset(\n",
    "        NPY_LOG_MEL_TRAIN_DIR_FOR_DATASET, train_filenames.tolist(), train_labels.tolist(),\n",
    "        mean=fold_mean, std=fold_std\n",
    "    )\n",
    "    val_dataset = LogMelAudioDataset(\n",
    "        NPY_LOG_MEL_TRAIN_DIR_FOR_DATASET, val_filenames.tolist(), val_labels.tolist(),\n",
    "        mean=fold_mean, std=fold_std\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    model = SingleBranchLogMelCNN(NUM_CLASSES, INPUT_CHANNELS, LOG_MEL_HEIGHT, LOG_MEL_WIDTH, FC_HIDDEN_DIM_MODEL, DROPOUT_RATE_MODEL).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "    current_best_model_path = os.path.join(MODEL_SAVE_DIR, f'logmel_skfold_model_fold_{fold}.pt')\n",
    "    early_stopper = EarlyStopping(patience=EARLY_STOPPING_PATIENCE, min_delta=EARLY_STOPPING_MIN_DELTA, path=current_best_model_path)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        for log_mels, labels in train_loader:\n",
    "            log_mels, labels = log_mels.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(log_mels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for log_mels, labels in val_loader:\n",
    "                log_mels, labels = log_mels.to(device), labels.to(device)\n",
    "                outputs = model(log_mels)\n",
    "                val_loss += criterion(outputs, labels).item() * log_mels.size(0)\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        if early_stopper.early_stop:\n",
    "            print(f\"Fold {fold+1}, Epoch {epoch+1}: Early stopping.\")\n",
    "            break\n",
    "    print(f\"Fold {fold+1} 학습 완료.\")\n",
    "\n",
    "print(f\"\\n{'='*20} Stratified K-Fold 학습 완료 {'='*20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060098d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 15개의 Fold 모델로 앙상블 추론을 시작합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kki88\\AppData\\Local\\Temp\\ipykernel_11600\\3209315936.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  inference_model.load_state_dict(torch.load(path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "앙상블 추론 완료.\n"
     ]
    }
   ],
   "source": [
    "model_paths = glob.glob(os.path.join(MODEL_SAVE_DIR, 'logmel_skfold_model_fold_*.pt'))\n",
    "if not model_paths:\n",
    "    print(\"오류: 학습된 K-Fold 모델 파일이 없습니다.\")\n",
    "else:\n",
    "    print(f\"총 {len(model_paths)}개의 Fold 모델로 앙상블 추론을 시작합니다.\")\n",
    "\n",
    "    df_test = pd.read_csv(TEST_CSV_FILE_PATH)\n",
    "    test_file_ids = df_test[FILENAME_COLUMN_IN_CSV].tolist()\n",
    "    test_dataset = LogMelAudioDataset(\n",
    "        npy_log_mel_base_dir=NPY_LOG_MEL_TEST_DIR_FOR_DATASET,\n",
    "        file_names_list=test_file_ids,\n",
    "        labels_list=None,\n",
    "        mean=overall_log_mel_mean,\n",
    "        std=overall_log_mel_std,   \n",
    "        is_test=True\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    all_model_probs = []\n",
    "    for path in model_paths:\n",
    "        inference_model = SingleBranchLogMelCNN(NUM_CLASSES, INPUT_CHANNELS, LOG_MEL_HEIGHT, LOG_MEL_WIDTH, FC_HIDDEN_DIM_MODEL, DROPOUT_RATE_MODEL).to(device)\n",
    "        inference_model.load_state_dict(torch.load(path, map_location=device))\n",
    "        inference_model.eval()\n",
    "        current_model_probs = []\n",
    "        with torch.no_grad():\n",
    "            for log_mels_test, _ in test_loader:\n",
    "                log_mels_test = log_mels_test.to(device)\n",
    "                logits = inference_model(log_mels_test)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                current_model_probs.append(probs.cpu())\n",
    "        all_model_probs.append(torch.cat(current_model_probs, dim=0))\n",
    "    ensembled_probs_tensor = torch.stack(all_model_probs)\n",
    "    avg_probs = ensembled_probs_tensor.mean(dim=0)\n",
    "    _, final_preds_indices = torch.max(avg_probs, 1)\n",
    "\n",
    "    idx_to_label = {v: k for k, v in label_to_int.items()}\n",
    "    predicted_labels_str = [idx_to_label[p_idx] for p_idx in final_preds_indices.tolist()]\n",
    "    current_time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    submission_filename = f'submission_logmel_skfold_ensemble_{current_time_str}.csv'\n",
    "    submission_save_path = os.path.join(SUBMISSION_SAVE_DIR, submission_filename)\n",
    "    submission_df = pd.DataFrame({'ID': test_file_ids, 'Target': predicted_labels_str})\n",
    "    submission_df.to_csv(submission_save_path, index=False)\n",
    "    print(f\"\\n앙상블 추론 완료.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
